options(java.parameters="-Xmx2g")

library(dfrtopics)
library(dplyr)

out_dir <- "/Users/agoldst/Documents/signs-model/instances/"
stoplist_file <- "/Users/agoldst/Documents/signs-model/modeling/stoplist/stop_refs.txt"

len_threshold <- 800
freq_threshold <- 4

# for reading the wordcount files generated by dfr_tokenize.py (either my variant
# or the files originally supplied by Luke H.

read_headless_counts <- function(fv,report_interval=100) {
    
    is_empty <- file.info(fv)$size == 0
    message("Skipping ",sum(is_empty)," empty files")
    fv <- fv[!is_empty]
    
    counts <- vector("list",length(fv))
    n_types <- integer(length(fv))
    
    for(i in seq_along(fv)) { 
        counts[[i]] <- read.csv(fv[i],strip.white=T,header=F,
                                col.names=c("WORDCOUNTS","WEIGHT"),
                                as.is=T,
                                colClasses=c("character","integer"))
        
        
        n_types[i] <- nrow(counts[[i]])
        
        if(i %% report_interval == 0) {
            message("Read ",i," files")
        }
    }
    
    message("Preparing aggregate data frame...")
    
    # infuriatingly, concatenating columns separately is a major
    # performance improvement
    
    wordtype <- do.call(c,lapply(counts,"[[","WORDCOUNTS"))
    wordweight <- do.call(c,lapply(counts,"[[","WEIGHT"))
    
    # add id column
    ids <- sub("^.*references_","",fv)
    ids <- sub("\\.txt.*$","",ids)
    ids <- gsub("_","/",ids)
    
    data.frame(id=rep(ids,times=n_types),
               WORDCOUNTS=wordtype,
               WEIGHT=wordweight,
               stringsAsFactors=F)
    
}

# experimenting with reference section counts

check_ref_counts <- function (counts) {
    # reference file counts: these turn out to be garbage in two ways.    
    # The tokenizing appears to discard quite a few terms, and the source   
    # text includes extra cruft from JSTOR's citation analyzer that yields, 
    # in some cases, a higher count for terms in the references than in the 
    # article text as a whole.                                              

    ref_files <- Sys.glob(file.path(data_dir,"hospadaruk","*.csv"))
    ref_counts <- read_headless_counts(ref_files)

    colnames(ref_counts)[3] <- "ref_weight"
    counts_combined <- counts %>%
        left_join(ref_counts,by=c("id","WORDCOUNTS")) %>%
        mutate(ref_weight=ifelse(is.na(ref_weight),0,ref_weight)) %>%
        mutate(adjusted_weight=WEIGHT - ref_weight)

    # return joined table for exploration
    counts_combined
}
    
read_dfr_fulltext <- function(data_dir,type) {
    fv <- Sys.glob(file.path(data_dir,type,"*.txt"))
    ids <- sub(paste("^.*",type,"_",sep=""), "", fv)
    ids <- sub("\\.txt$", "", ids)
    ids <- sub("_", "/", ids)

    texts <- character(length(ids))

    for (j in seq_along(fv)) {
        texts[j] <- paste(readLines(fv[j],warn=F),collapse=" ")
        if (j %% 100 == 0) {
            message("Read ",j," files")
        }
    }

    # rejoin hyphenation as in dfr_tokenize.py
    texts <- gsub("-\\s+","",texts,perl=T)

    # eliminate injected LaTeX; otherwise usepackage is a topic key word
    texts <- gsub("\\\\[\\w}{,\\[\\]]+","",texts,perl=T)

    data.frame(id=ids,text=texts,stringsAsFactors=F)
}

rare_words <- function(instances_nostop, threshold) {
    tdm <- instances_term_document_matrix(instances_nostop)
    term_freqs <- rowSums(tdm)

    # rank thresholding (unused here)
    # stolen from dplyr::desc and dplyr::n_top
    # ranks <- rank(-xtfrm(term_freqs),ties.method="min")
    # this reveals the following tail of the frequency distribution
    # > length(term_freqs)
    # [1] 181210
    # > tail(sort(unique(ranks)))
    # [1] 48226 52310 57624 65057 76651 99778
    # > head(sort(unique(term_freqs)))
    # [1] 1 2 3 4 5 6

    # instead, let's do a basic freq cutoff
    vocab <- instances_vocabulary(instances_nostop)
    tokens_kept <- sum(term_freqs[term_freqs > threshold]) 
    total_tokens <- sum(term_freqs)
    message("Of ",length(vocab)," terms, ",sum(term_freqs > threshold),
            " occur strictly more than ",threshold, "x\n",
            "or ",tokens_kept,
            " of ", total_tokens," (",tokens_kept / total_tokens, ") tokens.")
    vocab[term_freqs < threshold]
}

# G^2, Dunning's log likelihood statistic
g2 <- function (a, b) {
    stopifnot(length(a) == length(b))
    a_tot <- sum(a)
    b_tot <- sum(b)
    
    E_a <- (a + b) * a_tot / (a_tot + b_tot)
    E_b <- (a + b) * b_tot / (a_tot + b_tot)
    
    a_score <- ifelse(a > 0,a * log(a / E_a),0)
    b_score <- ifelse(b > 0,b * log(b / E_b),0)
    
    2 * (a_score + b_score)
}

# take a frame of docs and the unstopped instances
# in order to trim away unwanted documents:
# either those cut from the beta model
# or non-flas
# or under the length threshold
# or a few special cases

prune_docs <- function (docs, instances_nostop) {
    ids <- docs$id

    meta <- read_metadata(
        c("/Users/agoldst/Documents/signs-model/data/20140827/citations.tsv",
          "/Users/agoldst/Documents/signs-model/scrape_txt/citations.tsv"))

    stopifnot(all(ids %in% meta$id))

    # trim files

    ids_beta <- readLines("/Users/agoldst/Documents/signs-model/model65_fla_800/id_map.txt")

    meta_beta <- read_metadata("/Users/agoldst/Documents/signs-model/data/20131105/citations.CSV")

    exclusions_beta <- setdiff(meta_beta$id, ids_beta)

    exclusions_type <- meta$id[meta$type != "fla"]

    # to omit 2014 (incomplete in data set)
    # exclusions_date <- meta$id[pubdate_Date(meta$pubdate) < as.Date("2014-01-01")]
    exclusions_date <- character(0)

    # get document length (in tokens) from the mallet instances
    iter <- instances_nostop$iterator()
    instance_len <- function() {
        inst <- .jcall(iter,"Ljava/lang/Object;","next")
        fs <- .jcall(inst,"Ljava/lang/Object;","getData")
        .jcall(fs,"I","getLength")
    }

    doc_lens <- replicate(instances_nostop$size(),instance_len())

    exclusions_len <- ids[doc_lens < len_threshold]

    exclusions <- unique(c(exclusions_beta, exclusions_type,
                           exclusions_date, exclusions_len))

    # a few special cases, correcting issues with beta model set, etc.
    # duplicates: from 34.2 to 35.1, each item is doubled: one 10.1086 (which is
    # the article text only, and leads to a "full text" display on JSTOR) and
    # one 10.2307 (which is the "Page Scan" and is OCR'd text rather than born-
    # digital). 
    # Some of these were cut out by the length criterion (the zines articles), and 
    # the team caught most but not all of the others in the beta model.
    # Here are the rest (all in 34.2). We'll prefer the 10.1086 versions.
    dupes_keep <- c("10.1086/590976","10.1086/591189","10.1086/591249",
                    "10.1086/591086")
    dupes_discard <- c("10.2307/25195290", "10.2307/25195292", "10.2307/25195297",
                       "10.2307/25195298")

    # 10.1086/590976 Olga Voronina, "Has Feminist Philosophy a Future in Russia?," *Signs* 34, no. 2 (January 2009): 252-257.
    # 10.1086/591189 Charmaine Pereira, "Setting Agendas for Feminist Thought and Practice in Nigeria," *Signs* 34, no. 2 (January 2009): 263-269.
    # 10.1086/591249 Chikako Nagayama, "The Flux of Domesticity and the Exotic in a Wartime Melodrama," *Signs* 34, no. 2 (January 2009): 369-395.
    # 10.1086/591086 Sara Goodkind, "“You can be anything you want, but you have to believe it”: Commercialized Feminism in Gender‐Specific Programs for Girls," *Signs* 34, no. 2 (January 2009): 397-422.
    # 10.2307/25195290 Olga Voronina, "Has Feminist Philosophy a Future in Russia?," *Signs* 34, no. 2 (January 2009): 252-257.
    # 10.2307/25195292 Charmaine Pereira, "Setting Agendas for Feminist Thought and Practice in Nigeria," *Signs* 34, no. 2 (January 2009): 263-269.
    # 10.2307/25195297 Chikako Nagayama, "The Flux of Domesticity and the Exotic in a Wartime Melodrama," *Signs* 34, no. 2 (January 2009): 369-395.
    # 10.2307/25195298 Sara Goodkind, ""You Can Be Anything You Want, but You Have to Believe It": Commercialized Feminism in Gender-Specific Programs for Girls," *Signs* 34, no. 2 (January 2009): 397-422.

    exclusions <- c(exclusions, dupes_discard)
    stopifnot(!any(dupes_keep %in% exclusions))

    # editorial mistakenly in betamodel:
    # 10.1086/503787 "Mary Hawkesworth, \"Editorial: Legacies, Transitions, and New Directions at <i>Signs</i>,\" *Signs* 31, no. 4 (June 2006): 901-914."
    exclusions <- c(exclusions, "10.1086/503787") 

    # mistakenly excluded from betamodel
    # 10.2307/3173853 Catharine A. MacKinnon, "Feminism, Marxism, Method, and the State: An Agenda for Theory," *Signs* 7, no. 3 (April 1982): 515-544.
    # 10.2307/3173323 Donna Haraway, "Animal Sociology and a Natural Economy of the Body Politic, Part I: A Political Physiology of Dominance," *Signs* 4, no. 1 (October 1978): 21-36.
    exclusions <- setdiff(exclusions, c("10.2307/3173853", "10.2307/3173323"))

    # alas, the symposium in international languages (39.3) is an ocr problem
    exclusions <- c(exclusions, "10.1086/676576")

    # two "About the Contributors" marked as fla
    exclusions <- c(exclusions, "10.1086/674437","10.1086/673194")

    docs <- docs[!(docs$id %in% exclusions),]

    message("Kept ",nrow(docs),"of ",length(ids)," documents")

    docs
}


ref_words <- function (instances_nostop, stopfile) {
    refs <- read_dfr_fulltext("/Users/agoldst/Documents/signs-model/data/fulltext",
                              "references")

    instances_refs <- make_instances(refs,stopfile)

    docs_tdm <- instances_term_document_matrix(instances_nostop)
    refs_tdm <- instances_term_document_matrix(instances_refs)

    docs_vocab <- instances_vocabulary(instances_nostop)
    refs_vocab <- instances_vocabulary(instances_refs)
    shared_vocab <- intersect(docs_vocab,refs_vocab)
    docs_counts <- rowSums(docs_tdm[match(shared_vocab,docs_vocab),])
    refs_counts <- rowSums(refs_tdm[match(shared_vocab,refs_vocab),])

    ll <- g2(refs_counts,docs_counts)
    shared_vocab[order(ll, decreasing=T)[1:20]]
}



# regular DfR counts: these are identical in 20140827 and 20140819
# counts <- read_dfr(file.path(data_dir,"wordcounts"))
# unfortunately I've also lost faith in the tokenization here

empty_stop <- tempfile()
writeLines("",empty_stop)

docs <- read_dfr_fulltext("/Users/agoldst/Documents/signs-model/data/fulltext",
                          "ocr")
scraped <- read_dfr_fulltext("/Users/agoldst/Documents/signs-model/scrape_txt",
                             "ocr")
docs <- rbind(docs,scraped)

instances_nostop <- make_instances(docs,empty_stop)
rares <- rare_words(instances_nostop, freq_threshold)

# to get a list of distinctive words from the references:
# ref_words(instances_nostop, empty_stop)

pruned_docs <- prune_docs(docs, instances_nostop)

# ugly way to get mallet to prune those rare words: add them to a temporary stopfile
stops_rares <- tempfile()
writeLines(c(readLines(stoplist_file),rares),stops_rares)

instances <- make_instances(pruned_docs,stops_rares)

write_instances(instances, file.path(out_dir,"signs_fla_len800_stoprefs.mallet"))

unlink(empty_stop)
unlink(stops_rares)
